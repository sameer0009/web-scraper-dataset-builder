# üï∑Ô∏è Web Scraper & Dataset Builder

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://www.python.org/downloads/)
[![License](https://img.shields.io/badge/License-MIT-green.svg)](LICENSE)
[![Platform](https://img.shields.io/badge/Platform-Windows%20%7C%20Linux%20%7C%20macOS-lightgrey.svg)](https://github.com/sameer0009/web-scraper-dataset-builder)
[![Tests](https://img.shields.io/badge/Tests-Passing-brightgreen.svg)](tests/)

A comprehensive, industry-ready Python application for web scraping, data cleaning, and dataset creation with a modern graphical user interface. Built with enterprise-grade architecture, comprehensive error handling, and professional data processing capabilities.

![Application Demo](https://via.placeholder.com/800x400/2b2b2b/ffffff?text=Web+Scraper+%26+Dataset+Builder+Demo)

## üöÄ Quick Start

### One-Click Installation & Launch

**Windows:**
```cmd
# Simply double-click or run:
install_and_run.bat
```

**Linux/macOS:**
```bash
chmod +x install_and_run.sh
./install_and_run.sh
```

That's it! The script will automatically:
- ‚úÖ Check Python installation
- ‚úÖ Create virtual environment
- ‚úÖ Install all dependencies
- ‚úÖ Run tests to verify setup
- ‚úÖ Launch the application

## ‚ú® Features

### Web Scraping
- **Static Content Scraping**: BeautifulSoup4-powered extraction from HTML pages
- **Dynamic Content Support**: Selenium WebDriver for JavaScript-heavy sites
- **Intelligent Pagination**: Automatic detection and traversal of multi-page content
- **Robust Error Handling**: Network timeouts, retries, and graceful failure recovery
- **Configurable Delays**: Respectful scraping with customizable request intervals
- **Multiple Target Elements**: Tables, lists, divs, spans, and custom selectors

### Data Processing & Cleaning
- **Duplicate Management**: Multiple strategies (first, last, all) for duplicate removal
- **Missing Value Handling**: Drop, fill (mean/median/mode/custom), interpolation, KNN imputation
- **Text Processing**: Remove special characters, HTML tags, normalize whitespace, case conversion
- **Data Type Conversion**: Intelligent conversion between string, numeric, datetime, and categorical types
- **Column Management**: Rename, reorder, and restructure datasets
- **Outlier Detection**: IQR and Z-score methods for outlier identification and removal
- **Operation History**: Full undo/redo functionality with operation snapshots

### Export Capabilities
- **Excel (.xlsx)**: Professional formatting, multiple sheets, auto-sizing, freeze panes
- **CSV**: Configurable delimiters, encoding options, custom formatting
- **JSON**: Multiple orientations (records, index, values, split, table)
- **Parquet**: Efficient columnar storage with compression
- **HTML**: Styled tables with metadata
- **XML**: Structured data export
- **Batch Export**: Export to multiple formats simultaneously

### Project Management
- **Project Templates**: Pre-configured templates for common scraping scenarios
- **Save/Load Projects**: Complete project state persistence with versioning
- **Project Import/Export**: Portable project packages with ZIP compression
- **Recent Projects**: Quick access to recently used projects
- **Project Statistics**: Analytics and insights about your scraping projects
- **Backup System**: Automatic backups with configurable retention

### User Interface
- **Modern Design**: CustomTkinter-based interface with dark/light themes
- **Tabbed Navigation**: Intuitive workflow across Scraper, Cleaner, and Export tabs
- **Real-time Progress**: Progress bars and status updates for long operations
- **Interactive Data Tables**: Sortable, filterable data views with editing capabilities
- **Error Dialogs**: User-friendly error messages with actionable suggestions
- **Responsive Layout**: Adaptive interface that works on different screen sizes

## üõ†Ô∏è Installation

### Prerequisites

- **Python 3.10+** (Required)
- **Chrome/Chromium Browser** (For dynamic content scraping)
- **4GB RAM minimum** (8GB recommended for large datasets)
- **500MB disk space** (For application and dependencies)

### Quick Setup

1. **Clone the repository**:
   ```bash
   git clone <repository-url>
   cd web-scraper-dataset-builder
   ```

2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Run the application**:
   ```bash
   python main.py
   ```

### Advanced Installation

For development or advanced usage:

```bash
# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install with development dependencies
pip install -r requirements.txt

# Run tests
python test_comprehensive.py

# Run with debug logging
python main.py --log-level DEBUG
```

## üìñ Usage Guide

### Getting Started

1. **Launch the Application**: Run `python main.py`
2. **Create a New Project**: File ‚Üí New Project or use a template
3. **Configure Scraping**: Enter URL and set scraping parameters
4. **Start Scraping**: Click "Start Scraping" and monitor progress
5. **Clean Data**: Switch to Dataset Cleaner tab for data processing
6. **Export Results**: Choose format and export your cleaned dataset

### Web Scraper Tab

#### Basic Configuration
- **URL**: Target website URL (supports HTTP/HTTPS)
- **Target Elements**: HTML elements to extract (tables, divs, spans, etc.)
- **Max Pages**: Limit pagination traversal (1-100 pages)
- **Request Delay**: Delay between requests (0.5-10 seconds)

#### Advanced Options
- **Dynamic Scraping**: Enable for JavaScript-rendered content
- **Custom Headers**: Add custom HTTP headers
- **User Agent**: Customize browser identification
- **Timeout Settings**: Configure request timeouts
- **Retry Logic**: Set retry attempts for failed requests

#### Scraping Process
1. URL validation and accessibility check
2. Content type detection (static vs dynamic)
3. HTML parsing and data extraction
4. Pagination detection and traversal
5. Data consolidation and preview

### Dataset Cleaner Tab

#### Data Quality Assessment
- **Summary Statistics**: Rows, columns, data types, memory usage
- **Missing Value Analysis**: Count and percentage by column
- **Duplicate Detection**: Identify and highlight duplicate records
- **Data Type Validation**: Verify and suggest type corrections

#### Cleaning Operations
- **Remove Duplicates**: Choose strategy (first, last, none)
- **Handle Missing Values**: Multiple imputation strategies
- **Text Cleaning**: Remove HTML, special characters, normalize text
- **Data Type Conversion**: Convert between types with validation
- **Column Operations**: Rename, reorder, add, or remove columns
- **Outlier Management**: Detect and remove statistical outliers

#### Operation History
- **Undo/Redo**: Full operation history with snapshots
- **Operation Log**: Detailed log of all transformations
- **Before/After Preview**: Visual comparison of changes
- **Batch Operations**: Apply multiple operations in sequence

### Export Tab

#### Format Selection
- **Excel**: Professional spreadsheets with formatting
- **CSV**: Universal compatibility with custom delimiters
- **JSON**: Structured data for APIs and applications
- **Parquet**: Efficient storage for big data workflows

#### Export Configuration
- **File Location**: Choose save directory and filename
- **Format Options**: Customize encoding, delimiters, structure
- **Metadata Inclusion**: Add export timestamp and source information
- **Compression**: Enable compression for supported formats

#### Batch Export
- Export to multiple formats simultaneously
- Consistent naming conventions
- Progress tracking for large exports
- Error handling and recovery

## üèóÔ∏è Architecture

### Core Components

```
web-scraper-dataset-builder/
‚îú‚îÄ‚îÄ main.py                 # Application entry point
‚îú‚îÄ‚îÄ config.py              # Configuration management
‚îú‚îÄ‚îÄ models.py              # Data models and validation
‚îú‚îÄ‚îÄ ui.py                  # User interface components
‚îú‚îÄ‚îÄ scraper.py             # Web scraping engine
‚îú‚îÄ‚îÄ cleaner.py             # Data cleaning operations
‚îú‚îÄ‚îÄ export_manager.py      # Export functionality
‚îú‚îÄ‚îÄ project_manager.py     # Project management
‚îú‚îÄ‚îÄ utils/                 # Utility modules
‚îÇ   ‚îú‚îÄ‚îÄ logger.py          # Logging system
‚îÇ   ‚îî‚îÄ‚îÄ error_handler.py   # Error management
‚îú‚îÄ‚îÄ tests/                 # Test suite
‚îú‚îÄ‚îÄ requirements.txt       # Dependencies
‚îî‚îÄ‚îÄ README.md             # Documentation
```

### Design Patterns

- **Model-View-Controller (MVC)**: Clear separation of concerns
- **Factory Pattern**: Object creation and configuration
- **Observer Pattern**: Progress updates and event handling
- **Strategy Pattern**: Multiple algorithms for data processing
- **Command Pattern**: Undo/redo functionality

### Data Flow

1. **Input**: URL and configuration parameters
2. **Scraping**: HTML extraction and parsing
3. **Processing**: Data cleaning and transformation
4. **Storage**: Project persistence and caching
5. **Output**: Multi-format data export

## üß™ Testing

### Test Suite

Run the comprehensive test suite:

```bash
python test_comprehensive.py
```

### Test Coverage

- **Unit Tests**: Individual component testing
- **Integration Tests**: End-to-end workflow validation
- **Performance Tests**: Large dataset handling
- **Error Handling Tests**: Failure scenario validation
- **UI Tests**: Interface component testing

### Test Categories

- ‚úÖ Core Data Models
- ‚úÖ Web Scraping Engine
- ‚úÖ Data Cleaning Operations
- ‚úÖ Export Functionality
- ‚úÖ Project Management
- ‚úÖ Error Handling System
- ‚úÖ Integration Workflows
- ‚úÖ Performance Benchmarks

## üìä Performance

### Benchmarks

- **Small Datasets** (< 1K records): < 1 second processing
- **Medium Datasets** (1K-10K records): < 10 seconds processing
- **Large Datasets** (10K-100K records): < 60 seconds processing
- **Memory Usage**: Optimized for datasets up to 1GB
- **Export Speed**: 10K records/second average

### Optimization Features

- **Chunked Processing**: Handle large datasets efficiently
- **Memory Management**: Automatic garbage collection
- **Background Threading**: Non-blocking UI operations
- **Caching**: Intelligent caching of parsed content
- **Streaming**: Stream processing for large exports

## üîß Configuration

### Application Settings

Configuration is stored in platform-specific locations:

- **Windows**: `%APPDATA%/Web Scraper & Dataset Builder/config.json`
- **macOS**: `~/Library/Application Support/Web Scraper & Dataset Builder/config.json`
- **Linux**: `~/.config/web-scraper-dataset-builder/config.json`

### Customizable Options

```json
{
  "scraping": {
    "max_pages": 10,
    "delay_between_requests": 1.0,
    "timeout": 30,
    "max_retries": 3,
    "user_agent": "Custom User Agent"
  },
  "ui": {
    "theme": "dark",
    "window_width": 1200,
    "window_height": 800,
    "auto_save_projects": true
  },
  "export": {
    "default_format": "xlsx",
    "include_index": false,
    "encoding": "utf-8"
  }
}
```

## üö® Error Handling

### Error Categories

- **Network Errors**: Connection issues, timeouts, HTTP errors
- **Parsing Errors**: Invalid HTML, encoding issues
- **Data Errors**: Type conversion, memory issues
- **File Errors**: Permission denied, disk space
- **Validation Errors**: Invalid input, configuration errors

### Recovery Mechanisms

- **Automatic Retry**: Exponential backoff for network requests
- **Graceful Degradation**: Partial data extraction on errors
- **User Guidance**: Clear error messages with suggested actions
- **State Recovery**: Automatic save/restore of work progress
- **Logging**: Comprehensive error logging for debugging

## üîí Security & Privacy

### Data Protection

- **Local Processing**: All data processing happens locally
- **No Data Transmission**: No data sent to external servers
- **Secure Storage**: Encrypted configuration storage
- **Privacy Compliance**: Respects robots.txt and rate limits

### Best Practices

- **Rate Limiting**: Respectful scraping with delays
- **User Agent**: Proper browser identification
- **SSL Verification**: Secure HTTPS connections
- **Input Validation**: Sanitized user inputs
- **Error Sanitization**: No sensitive data in error messages

## ü§ù Contributing

### Development Setup

1. Fork the repository
2. Create a feature branch
3. Install development dependencies
4. Run tests before committing
5. Submit a pull request

### Code Standards

- **PEP 8**: Python style guide compliance
- **Type Hints**: Full type annotation
- **Documentation**: Comprehensive docstrings
- **Testing**: Unit tests for new features
- **Error Handling**: Proper exception management

## üìÑ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## üÜò Support

### Getting Help

- **Documentation**: Check this README and inline documentation
- **Issues**: Create GitHub issues for bugs and feature requests
- **Discussions**: Use GitHub Discussions for questions
- **Email**: Contact the development team

### Common Issues

1. **Chrome Driver Issues**: Ensure Chrome/Chromium is installed
2. **Memory Errors**: Reduce dataset size or increase system memory
3. **Network Timeouts**: Increase timeout settings or check connectivity
4. **Permission Errors**: Run with appropriate file system permissions

### System Requirements

- **Minimum**: Python 3.10, 4GB RAM, 500MB disk space
- **Recommended**: Python 3.11+, 8GB RAM, 2GB disk space
- **Optimal**: Python 3.11+, 16GB RAM, SSD storage

---

**Built with ‚ù§Ô∏è for the data science community**

*Web Scraper & Dataset Builder - Making web data extraction accessible to everyone*